{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems using Deep Autoencoders\n",
    "\n",
    "In this lab, you'll learn how to generate recommendations for users based on ratings they've provided for other items. \n",
    "\n",
    "### Getting started\n",
    "\n",
    "First, let's check what GPUs we have on our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 16 21:21:02 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla M40           On   | 00000000:04:00.0 Off |                    0 |\r\n",
      "|  0%   23C    P8    19W / 250W |     11MiB / 11443MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see our system has 1 GPU, a V100 with a PCIE link.\n",
    "\n",
    "Additionally, let's examine our file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 85740\r\n",
      "drwxr-xr-x 16 34021 1001     4096 Feb 16 21:19 .\r\n",
      "drwxr-xr-x 13 34021 1001     4096 Feb 14 19:01 ..\r\n",
      "-rw-r--r--  1 34021 1001     8196 Jan 26 17:50 .DS_Store\r\n",
      "drwxr-xr-x  7 34021 1001     4096 Feb 16 21:18 .git\r\n",
      "-rw-r--r--  1 34021 1001      150 Feb 15 21:20 .gitignore\r\n",
      "drwxr-xr-x  4 34021 1001     4096 Jan 26 18:51 .idea\r\n",
      "drwxr-xr-x  2 34021 1001     4096 Feb 15 21:34 .ipynb_checkpoints\r\n",
      "-rw-r--r--  1 34021 1001   101983 Feb 16 21:18 Answers.ipynb\r\n",
      "-rw-r--r--  1 34021 1001    42136 Feb 16 21:18 DLI Lab - Stretch.ipynb\r\n",
      "-rw-r--r--  1 34021 1001    46803 Feb 16 21:19 DLI Lab.ipynb\r\n",
      "-rw-r--r--  1 34021 1001      528 Jan 26 17:50 Dockerfile\r\n",
      "-rw-r--r--  1 34021 1001     1075 Jan 26 17:50 LICENSE\r\n",
      "drwxr-xr-x 14 34021 1001     4096 Feb  2 16:01 Netflix\r\n",
      "-rw-r--r--  1 34021 1001 44496853 Feb  8 15:46 Online Retail.csv\r\n",
      "-rw-r--r--  1 34021 1001     3428 Jan 26 17:50 README.md\r\n",
      "drwxr-xr-x  2 34021 1001     4096 Feb  2 15:59 __pycache__\r\n",
      "drwxr-xr-x  4 34021 1001     4096 Jan 26 18:51 azkaban\r\n",
      "-rw-r--r--  1 34021 1001      947 Jan 26 17:50 compute_RMSE.py\r\n",
      "drwxr-xr-x  2 34021 1001     4096 Feb  5 23:48 data_utils\r\n",
      "drwxr-xr-x  2 34021 1001     4096 Feb 15 16:30 images\r\n",
      "-rw-r--r--  1 34021 1001     4253 Jan 26 17:50 infer.py\r\n",
      "-rw-r--r--  1 34021 1001     2353 Jan 26 17:50 logger.py\r\n",
      "drwxr-xr-x  2 34021 1001     4096 Feb 15 17:17 model_save\r\n",
      "-rw-r--r--  1 34021 1001 35530410 Feb 15 17:28 preds.txt\r\n",
      "-rw-r--r--  1 34021 1001  7066111 Feb  8 19:58 preds_train.txt\r\n",
      "drwxr-xr-x  5 34021 1001     4096 Jan 26 20:00 reco_encoder\r\n",
      "drwxr-xr-x  4 34021 1001     4096 Feb  6 00:08 retail\r\n",
      "-rw-r--r--  1 34021 1001    10425 Feb  5 21:51 run.py\r\n",
      "-rw-r--r--  1 34021 1001      403 Jan 31 16:59 run_notebook.sh\r\n",
      "drwxr-xr-x  2 34021 1001     4096 Jan 26 18:51 scripts\r\n",
      "drwxr-xr-x  4 34021 1001     4096 Jan 26 18:51 test\r\n",
      "drwxr-xr-x  5 34021 1001     4096 Feb  5 20:46 yoochoose-data\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see several folders, Jupyter notebooks, and Python scripts. We'll visit each of these later in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications and Use Cases\n",
    "\n",
    "The digital space is an important place for businesses to interact with their customers. In such a vast space and with today's wealth of information, customers are increasingly looking to businesses to help provide guidance on what choices they should make. Recommendation systems are able to provide this guidance; they are systems that help predict the “rating” or “preference” based on the rating provided by users of an item. They’ve become very popular in recent years, and are used in many areas such as:\n",
    "\n",
    "* **Media and Entertainment** - movies, music, news, and videos, \n",
    "* **Retail** - clothes, books, and other products, \n",
    "* **Social Media** - friends, celebrities to follow, groups, restaurants, and ads. \n",
    "\n",
    "There are several ways to implement recommender systems, the most common of which is the Collaborative Filtering approach. This approach examines the ratings users have given items, and leverages the groups collaborative ratings to predict missing ratings. Consider the following example: A user named John has rated Die Hard as 5 stars and Die Hard 2 as 4 stars. Another user named Matt has rated Die Hard as 5 stars but has not rated Die Hard 2. Using the information we know about John's ratings for Die Hard and Die Hard 2, we might infer that Matt would rate Die Hard 2 as 4 stars as well. This example is obviously very simple but it demonstrates the essence of collaborative filtering approaches; we are able to use many users' ratings of items to infer missing ratings.\n",
    "\n",
    "Another approach to recommender systems is to use users' ratings of items in a collaborative fashion as well as information about the users or items. For example, if John rated several movies in the Romantic Comedy genre postively such as The Proposal or When Harry Met Sally, we might recommend the movie Pretty Women. These approaches are known as Content-based recommender systems, and can leverage a wide variety of rich data sources such as transactional data, user and item attributes and sub-attributes, and user-item ratings.\n",
    "\n",
    "Lastly, another common approach, Knowledge-based recommender systems, applies to items that are purchased on an infrequent basis. For example, an individual might purchase a computer every couple of years. With these approaches, the users specifies requirements and preferences, and that specification is combined with domain knowledge to generate recommendations. \n",
    "\n",
    "In this lab, we will focus on collaborative filtering approaches.\n",
    "\n",
    "### Collaborative Filtering Apporaches\n",
    "\n",
    "Collaborative Filtering approaches can be classified into two types of methods:\n",
    "\n",
    "* **Memory-based methods**: These methods approach the problem by identifying a target customer Jane and identifying customers who are most similar to Jane. We can take these similar customers and combine their ratings for various items, and use those ratings to infer which items to recommend for Jane. Examples of these methods are nearest neighbor approaches using Pearson's correlation or cosine distance, or k-nearest neighbors approaches. Memory-based methods tend to be more heuristical, and therefore, their results can sometimes be easy to interpret. However, as your dataset grows in size, these memory based approaches may not be able use all the information available (imagine k-nearest neighbors with a large k), and often work poorly with sparse datasets with a large cardinality of items due to the curse of dimensionality.\n",
    "* **Model-based methods**: The methods approach the problem by specifying a set of inputs (often the users-items ratings matrix) and specifying an output (often the missing or unobserved ratings for a particular user) and work to identify a mapping from the inputs to the outputs. These methods often follow data mining and machine learning paradigms by parameterizing the mapping and use some optimization technique to minimize the error and identify the best parameters for that mapping. Examples of these approaches may be descision trees, Bayesian methods, or the deep autoencoders we will use in this lab.\n",
    "\n",
    "In general, collaborative filtering approach can be impeded by two challenges:\n",
    "\n",
    "* **Scalability**: Training may occur on millions of users and hundreds of thousands of items (or more!). Designing an algorithm that can be trained using this large amount of data and provide low-latency inference in a production system is key to a successful recommendation engine. \n",
    "* **Sparsity**: There may be hundreds of thousands of items, and users can only rate so many items. Resultingly, the data for users may be very sparse, and this consideration must be kept in mind when selecting the appropriate algorithm.\n",
    "\n",
    "This lab proposes using Deep Autoencoders to do Collaborative Filtering. We also propose a new training algorithm based on iterative ouput re-feeding to overcome naural sparseness of collaborate filtering. This new algorithm significally speeds up training and improves model performance, outperforming previous state-of-the art models on a time split Netflix data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Autoencoders\n",
    "\n",
    "Autoencoders are neural networks that are trained to attempt to copy its input to its output. Essentially, an autoencoder is attempting to approximate the identity function $f(x) = x$ such that the error between $f(x)$ and $x$ is minimized, under some constraints. These constraints are usually that the hidden layers, or latent space, are lower dimensional than the inputs. An autoencoder whose code dimension is less than the input dimension is called undercomplete. This constraint prevents the autoencoder from merely memorizing the data, and forces the autoencoder to learn efficient representations of the data, called \"codings\", that contain the patterns and features of the data. \n",
    "\n",
    "An autoencoder is composed of two parts; an encoder, and a decoder. The encoder maps the data to the hidden layer, or codings, and the decoder maps the codings back to the inputs. \n",
    "\n",
    "Autoencoders are an excellent tool for dimensionality reduction and can be thought of as a strict generalization of principle component analysis (PCA). An autoencoder without non-linear activations and only with “code” layer should be able to learn PCA transformation in the encoder if trained to optimize mean squared error (MSE) loss.\n",
    "\n",
    "![AutoEncoder](images/AutoEncoder.png)\n",
    "\n",
    "### MNIST example\n",
    "\n",
    "Autoencoders are fairly easy to implement. Below we show an implementation using the popular TensorFlow framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0 Test MSE: 0.025010614\n",
      "Epoch: 1 Test MSE: 0.023048468\n",
      "Epoch: 2 Test MSE: 0.0218137\n",
      "Epoch: 3 Test MSE: 0.021268478\n",
      "Epoch: 4 Test MSE: 0.021533297\n",
      "Epoch: 5 Test MSE: 0.021676585\n",
      "Epoch: 6 Test MSE: 0.021387115\n",
      "Epoch: 7 Test MSE: 0.021400116\n",
      "Epoch: 8 Test MSE: 0.021368964\n",
      "Epoch: 9 Test MSE: 0.020758744\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fff8ea670f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fff8a89e588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Image\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# matplotlib settings\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# network settings\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 128\n",
    "n_hidden2 = 64\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.01\n",
    "\n",
    "# define placeholder\n",
    "X_placeholder = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "# define graph\n",
    "hidden1 = fully_connected(X_placeholder, n_hidden1)\n",
    "hidden2 = fully_connected(hidden1, n_hidden2)\n",
    "hidden3 = fully_connected(hidden2, n_hidden3)\n",
    "outputs = fully_connected(hidden3, n_outputs)\n",
    "loss = tf.reduce_mean(tf.square(outputs - X_placeholder))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# create a node to initialize our global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# training settings\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "X_test = mnist.test.images\n",
    "\n",
    "# execute graph\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for n_epoch in range(n_epochs):\n",
    "        for n_batch in range(n_batches):\n",
    "            X_batch, _ = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_op, feed_dict={X_placeholder: X_batch})\n",
    "        loss_eval = loss.eval(feed_dict={X_placeholder: X_test})\n",
    "        print('Epoch:', n_epoch, 'Test MSE:', loss_eval)\n",
    "    \n",
    "    # show random test example\n",
    "    X_test, _ = mnist.test.next_batch(1)\n",
    "    fig1 = plt.figure()\n",
    "    plt.imshow(X_test.reshape((28, 28)), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.show()\n",
    "    print('Original Image')\n",
    "    fig2 = plt.figure()\n",
    "    X_test_eval = sess.run(outputs, feed_dict={X_placeholder: X_test})\n",
    "    plt.imshow(X_test_eval.reshape((28, 28)), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.show()\n",
    "    print('Reconstructed Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other forms of autoencoders exist and are described below:\n",
    "\n",
    "* **Constrained Autoencoder** - Autoencoders implement an encoding layer mapping the input to a codings layer, and a decoding layer mapping the docings back to the inputs. If the decoder architecture mirrors the encoder architecture, then one can constrain decoder’s weights to be equal to the transposed encoder weights for each mirrored layer. Such an autoencoder is called constrained or tied and has almost two times less free parameters than unconstrained one.\n",
    "* **Denoising Autoencoder** - Autoencoders can be used to remove noise from images. Typically, this is done by taking the input and adding a little bit of random noise to it; this noisy input is passed into the model while keeping the output noise-free. The autoencoder learns to map the noisy images to the noise-free images, and essentially learns how to remove noise from an image. \n",
    "* **Sparse Autoencoder** - Sparse autoencoders are autoencoders whose loss function is a combination of the reconstruction loss and some sparsity loss. By imposing this sparsity loss, we regularize the autoencoder and force it to learn more robust features of the data. Typically, sparse autoencoders are utilized as a means for learning the latent space of data; this latent space is later used as the input in another task such as regression or classification. \n",
    "* **Variational Autoencoder** - Variational autoencoders are a twist on traditional autoencoders. Instead of learning a deterministic codings layer, variational autoencoders learn a stochastic codings layer. This layer often comes in the form of a multivariate Gaussian distribution, where the means and sigma are learned from the data. This twist allows our model to take a generative flavor; by passing random Gaussian noise into the codings layer, we can generate entirely new images that look realisitc. For an example application of this concept, check out a variational autoencoder applied to celebrity faces: http://research.nvidia.com/publication/2017-10_Progressive-Growing-of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1) Implement a constrained autoencoder for the MNIST dataset. \n",
    "\n",
    "Hint: it may be easier to implement the weights and biases by hand instead of using TensorFlow's default `fully_connected` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement a denoising autoencoder for the MNIST dataset. \n",
    "\n",
    "Hint: think about how to modify your graph so as to accomodate noisy inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Implement a sparse autoencoder for the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Implement a variational autoencoder for the MNIST dataset. Can you sample from the codings to generate new images of digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Autoencoders learn a set of codings that represent the data i.e. it attempts to learn the underlying probability distribution generating the data. What happens if we introduce data from a new probability distribution? For example, what happens if instead of a digit between 0 and 9, we introduce the letter A? Or random noise? Choose one of your previous implementations of an autoencoder and pass in an image from another dataset. What does the reconstruction loss tell us? How can we use this concept to detect anomalies in our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and results\n",
    "\n",
    "### Dataset description\n",
    "\n",
    "The Netflix dataset is a well known and historically significant dataset, as its publication and prize from the resulting contest motivated a large body of work on recommendation systems. First published in October 2, 2006, the contest ran until September 21, 2009, when a reward of $1,000,000 was rewarded to team \"BellKor’s Pragmatic Chaos\". More information on their winning model can be found here: https://netflixprize.com/community/topic_1537.html\n",
    "\n",
    "This dataset contains users, items, and ratings that users gave to the items. The goal is to predict the \"missing\" or \"unobserved\" ratings in a separate test dataset. More information about the contest and the dataset can be found here:\n",
    "\n",
    "https://netflixprize.com/\n",
    "\n",
    "The raw datasets can be downloaded here (note these have already been downloaded and processed for you to save time):\n",
    "\n",
    "http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a\n",
    "\n",
    "For the ratings prediction task, it is often most relevant to predict future ratings given the past ones instead of predicting ratings missing at random. For evaluation purposes we split the original Netflix Prize training set into several training and testing intervals based on time. Training interval contains ratings which came in earlier than the ones from testing interval.\n",
    "\n",
    "The testing interval is then randomly split into Test and Validation subsets so that each rating from testing interval has a 50% chance of appearing in either subset. Users and items that do not appear in the training set are removed from both test and validation subsets.\n",
    "\n",
    "The data has already been downloaded and pre-processed for you. Below is more information on the number of ratings, users, and items in the train and test splits for each time period.\n",
    "\n",
    "\n",
    "| Dataset  | Netflix 3 months | Netflix 6 months | Netflix 1 year | Netflix full |\n",
    "| -------- | ---------------- | ---------------- | ----------- |  ------------ |\n",
    "| Ratings train | 13,675,402 | 29,179,009 | 41,451,832 | 98,074,901 |\n",
    "| Users train | 311,315 |390,795  | 345,855 | 477,412 |\n",
    "| Items train | 17,736 |17,757  | 16,907 | 17,768 |\n",
    "| Time range train | 2005-09-01 to 2005-11-31 | 2005-06-01 to 2005-11-31 | 2004-06-01 to 2005-05-31 | 1999-12-01 to 2005-11-31\n",
    "| -------- | ---------------- | ----------- |  ------------ |\n",
    "| Ratings test | 2,082,559 | 2,175,535  | 3,888,684| 2,250,481 |\n",
    "| Users test | 160,906 | 169,541  | 197,951| 173,482 |\n",
    "| Items test | 17,261 | 17,290  | 16,506| 17,305 |\n",
    "| Time range test | 2005-12-01 to 2005-12-31 | 2005-12-01 to 2005-12-31 | 2005-06-01 to 2005-06-31 | 2005-12-01 to 2005-12-31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the data. We'll use the `head` Linux command to inspect the first several rows of our train and test dataset from the Netflix 3 Months time split. The first column on the left hand side represents our User IDs; the second column represents our Item IDs; and the third and final column represents the rating that user gave to that time, on an ordinal scale of 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t4053\t3.0\r\n",
      "0\t2764\t4.0\r\n",
      "0\t3524\t4.0\r\n",
      "0\t5639\t4.0\r\n",
      "0\t12319\t3.0\r\n",
      "0\t12369\t4.0\r\n",
      "0\t14832\t4.0\r\n",
      "0\t22\t4.0\r\n",
      "0\t952\t5.0\r\n",
      "0\t1030\t3.0\r\n"
     ]
    }
   ],
   "source": [
    "!head Netflix/N3M_TRAIN/n3m.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1875\t3.0\r\n",
      "0\t14274\t5.0\r\n",
      "0\t5583\t4.0\r\n",
      "2\t16411\t2.0\r\n",
      "3\t7149\t3.0\r\n",
      "4\t5085\t3.0\r\n",
      "6\t383\t3.0\r\n",
      "6\t4640\t5.0\r\n",
      "7\t81\t3.0\r\n",
      "7\t849\t3.0\r\n"
     ]
    }
   ],
   "source": [
    "!head Netflix/N3M_TEST/n3m.test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The first application of autoencoders to collaborative filtering appeared in \"AutoRec: Autoencoders Meet Collaborative Filtering in 2015\". The idea is that we have $m$ users and $n$ items, and a matrix $R$ of shape $m$ x $n$ populated with ratings. The rating $r_{ij}$ details the rating given by the *ith* user to the *jth* item. The goal is to infer the missing entries in $R$. The ratings matrix is fed row-by-row into the autoencoder and the reconstruction loss calculated. For more information on U-AutoRec, please see the original paper: http://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf\n",
    "\n",
    "Our model is inspired by the U-AutoRec approach with several important distinctions. We train much deeper models, and do not use any layer-wise pre-training. We are able to do so successfully because we: \n",
    "\n",
    "1. use “scaled exponential linear units” (SELUs), \n",
    "2. use high dropout rates, and \n",
    "3. use iterative output re-feeding during training. \n",
    "\n",
    "For most of our experiments we use a batch size of 128, trained using SGD with momentum of 0.9 and learning rate of 0.001, and used Xavier initialization to initialize the parameters. \n",
    "\n",
    "In our model, both the encoder and decoder parts of the autoencoder consist of feed-forward neural networks with classical fully connected layers computing $l = f (W ∗ x + b)$, where $f$ is some non-linear activation function. If the range of the activation function is smaller than that of data, the last layer of the decoder should be kept linear. We find it to be very important for the activation function in the hidden layers to contain non-zero negative part, motivating the use SELU units in most of our experiments.\n",
    "\n",
    "For more information, please see the original paper here: https://arxiv.org/pdf/1708.01715.pdf\n",
    "\n",
    "For the actual code, please see the codebase here: https://github.com/NVIDIA/DeepRecommender\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Since it doesn’t make sense to predict zeros in user’s representation vector $x$, we instead optimize Masked Mean Squared Error loss:\n",
    "\n",
    "\\begin{equation*}\n",
    "MMSE   = \\frac{m_i * (r_i - y_i)^2}{\\sum_{i=1}^{n} m_i}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "\n",
    "* $r_i$ is actual rating, \n",
    "* $y_i$ is reconstructed, or predicted rating, \n",
    "* $m_i$ is a mask function such that $m_i = 1$ if $r_i \\neq 0$, else $m_i = 0$.\n",
    "\n",
    "Note that there is a straightforward relation between RMSE score and MMSE score: $RMSE = \\sqrt{MMSE}$.\n",
    "\n",
    "### Dense re-feeding\n",
    "\n",
    "During training and inference, an input $x ∈ R^n$ is very sparse because no user can realistically rate all items (especially if the cardinality of items is very large). On the other hand, the autoencoder’s output $f(x)$ is dense. \n",
    "\n",
    "Consider an idealized scenario with a *perfect* $f$, where by perfect $f$ we mean that $f(x)_i$ accurately predicts all user’s future ratings for items $i : x_i = 0$. Additionally, consider the situation where a user has rated all items and $x$ is dense; i.e. $\\forall i, x_i \\neq 0$. In this idealized scenario, $f(x)_i = x_i, ∀ i : x_i$. \n",
    "\n",
    "This means that that if a user rates a new item k (thereby creating a new vector $x^\\prime$) then $f(x^\\prime)_k = x^\\prime_k$ and $f(x^\\prime) = f(x^\\prime)$. Hence, in this idealized scenario, $y = f(x)$ should be a fixed point of a well trained autoencoder: $f(y) = y$. \n",
    "\n",
    "To explicitly enforce the fixed-point constraint and to be able to perform dense training updates, we augment every optimization iteration with an iterative dense re-feeding steps (3 and 4 below) as follows:\n",
    "\n",
    "1. Given a sparse input $x$, compute dense $f(x)$ and the loss detailed above (forward pass)\n",
    "2. Compute gradients and perform weight update (backward pass)\n",
    "3. Treat $f(x)$ as a new example and compute $f(f(x))$. Now both $f(x)$ and $f(f(x))$ are dense and the loss function detailed above has all m as non-zeros. (second forward pass)\n",
    "4. Compute gradients and perform weight update (second backward pass)\n",
    "\n",
    "Steps 3 and 4 can also be performed more than once for every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train our autoencoder on our data, we'll use the `run.py` script located in our current working directory. We can pass arguments to this script specifying the path to the location of the training and test data, the architecture of our model, and general settings for logging. To run this script in this Jupyter notebook, we can use the command `!python run.py <insert parameters here>`. Execute the cell below to train our autoencoder with 3 hidden layers of 128, 128, and a final codings layer of 256 on the Netflix 3 Month dataset for 3 epochs. Note that training this model may take up to 10 minutes depending on our hardware setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(aug_step=1, batch_size=128, constrained=False, drop_prob=0.8, gpu_ids='0', hidden_layers='128,128,256', logdir='model_save', lr=0.005, noise_prob=0.0, non_linearity_type='selu', num_epochs=3, optimizer='momentum', path_to_eval_data='Netflix/N3M_TEST', path_to_train_data='Netflix/N3M_TRAIN', skip_last_layer_nl=False, summary_frequency=500, weight_decay=0.0)\n",
      "Loading training data\n",
      "Data loaded\n",
      "Total items found: 311315\n",
      "Vector dim: 17736\n",
      "Loading eval data\n",
      "******************************\n",
      "******************************\n",
      "[17736, 128, 128, 256]\n",
      "Dropout drop probability: 0.8\n",
      "Encoder pass:\n",
      "torch.Size([128, 17736])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128])\n",
      "torch.Size([256])\n",
      "Decoder pass:\n",
      "torch.Size([128, 256])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128])\n",
      "torch.Size([17736, 128])\n",
      "torch.Size([17736])\n",
      "******************************\n",
      "******************************\n",
      "######################################################\n",
      "######################################################\n",
      "############# AutoEncoder Model: #####################\n",
      "AutoEncoder(\n",
      "  (drop): Dropout(p=0.8)\n",
      "  (encode_w): ParameterList(\n",
      "  )\n",
      "  (encode_b): ParameterList(\n",
      "  )\n",
      "  (decode_w): ParameterList(\n",
      "  )\n",
      "  (decode_b): ParameterList(\n",
      "  )\n",
      ")\n",
      "######################################################\n",
      "######################################################\n",
      "Using GPUs: [0]\n",
      "Doing epoch 0 of 3\n",
      "[0,     0] RMSE: 3.7730270\n",
      "[0,   500] RMSE: 1.5001395\n",
      "[0,  1000] RMSE: 1.0953987\n",
      "[0,  1500] RMSE: 1.0463029\n",
      "[0,  2000] RMSE: 1.0269772\n",
      "Total epoch 0 finished in 23.850396394729614 seconds with TRAINING RMSE loss: 1.1576488192647238\n",
      "Epoch 0 EVALUATION LOSS: 0.9998886920200939\n",
      "Saving model to model_save/model.epoch_0\n",
      "Doing epoch 1 of 3\n",
      "[1,     0] RMSE: 1.0163199\n",
      "[1,   500] RMSE: 1.0073406\n",
      "[1,  1000] RMSE: 1.0000420\n",
      "[1,  1500] RMSE: 0.9990173\n",
      "[1,  2000] RMSE: 0.9879501\n",
      "Total epoch 1 finished in 23.47741413116455 seconds with TRAINING RMSE loss: 0.9976235971883028\n",
      "Doing epoch 2 of 3\n",
      "[2,     0] RMSE: 0.9928149\n",
      "[2,   500] RMSE: 0.9793564\n",
      "[2,  1000] RMSE: 0.9801016\n",
      "[2,  1500] RMSE: 0.9932952\n",
      "[2,  2000] RMSE: 0.9950942\n",
      "Total epoch 2 finished in 22.853185892105103 seconds with TRAINING RMSE loss: 0.9854293179895487\n",
      "Epoch 2 EVALUATION LOSS: 0.979880978714129\n",
      "Saving model to model_save/model.epoch_2\n",
      "Saving model to model_save/model.last\n",
      "456 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "!python run.py --gpu_ids 0 \\\n",
    "--path_to_train_data Netflix/N3M_TRAIN \\\n",
    "--path_to_eval_data Netflix/N3M_TEST \\\n",
    "--hidden_layers 128,128,256 \\\n",
    "--non_linearity_type selu \\\n",
    "--batch_size 128 \\\n",
    "--logdir model_save \\\n",
    "--drop_prob 0.8 \\\n",
    "--optimizer momentum \\\n",
    "--lr 0.005 \\\n",
    "--weight_decay 0 \\\n",
    "--aug_step 1 \\\n",
    "--noise_prob 0 \\\n",
    "--num_epochs 3 \\\n",
    "--summary_frequency 500\n",
    "end = time.time()\n",
    "print(int(end - start), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize our model's losses, weights, and biases using TensorBoard. Execute the cell below to start running TensorBoard, and visit the URL running this notebook; however, instead of using port 8888 you will use port 6006. Your URL should look like this: http://inserturlhere:6006\n",
    "\n",
    "Note that this cell will run continuously, and you won't be able to execute any other cells in this notebook until you stop running the below cell. To do so, highlight the cell block and press the stop button (shaped in a solid square) next to the Run button at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=model_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "The next step in our deep learning workflow is to use our trained model to infer the missing ratings on our test dataset. This will allow us to objectively evaluate the quality of our model. The `infer.py` script will accomplish this objective, and we pass in several arguments again specifying the location of our data and our model architecture as well as the the location of the file we would like our predictions saved in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(constrained=False, drop_prob=0.8, hidden_layers='128,128,256', non_linearity_type='selu', path_to_eval_data='Netflix/N3M_TEST', path_to_train_data='Netflix/N3M_TRAIN', predictions_path='preds.txt', save_path='model_save/model.epoch_4', skip_last_layer_nl=False)\n",
      "Loading training data\n",
      "Data loaded\n",
      "Total items found: 311315\n",
      "Vector dim: 17736\n",
      "Loading eval data\n",
      "******************************\n",
      "******************************\n",
      "[17736, 128, 128, 256]\n",
      "Dropout drop probability: 0.8\n",
      "Encoder pass:\n",
      "torch.Size([128, 17736])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128])\n",
      "torch.Size([256])\n",
      "Decoder pass:\n",
      "torch.Size([128, 256])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128])\n",
      "torch.Size([17736, 128])\n",
      "torch.Size([17736])\n",
      "******************************\n",
      "******************************\n",
      "Loading model from: model_save/model.epoch_4\n",
      "######################################################\n",
      "######################################################\n",
      "############# AutoEncoder Model: #####################\n",
      "AutoEncoder(\n",
      "  (drop): Dropout(p=0.8)\n",
      "  (encode_w): ParameterList(\n",
      "  )\n",
      "  (encode_b): ParameterList(\n",
      "  )\n",
      "  (decode_w): ParameterList(\n",
      "  )\n",
      "  (decode_b): ParameterList(\n",
      "  )\n",
      ")\n",
      "######################################################\n",
      "######################################################\n",
      "Done: 0\n",
      "Done: 10000\n",
      "Done: 20000\n",
      "Done: 30000\n",
      "Done: 40000\n",
      "Done: 50000\n",
      "Done: 60000\n",
      "Done: 70000\n",
      "Done: 80000\n",
      "Done: 90000\n",
      "Done: 100000\n",
      "Done: 110000\n",
      "Done: 120000\n",
      "204 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "!python infer.py \\\n",
    "--path_to_train_data Netflix/N3M_TRAIN \\\n",
    "--path_to_eval_data Netflix/N3M_TEST \\\n",
    "--hidden_layers 128,128,256 \\\n",
    "--non_linearity_type selu \\\n",
    "--save_path model_save/model.epoch_2 \\\n",
    "--drop_prob 0.8 \\\n",
    "--predictions_path preds.txt\n",
    "end = time.time()\n",
    "print(int(end - start), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's compute our error. We'll use the `compute_RMSE.py` script and pass in the location of our predictions file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(path_to_predictions='preds.txt', round=False)\n",
      "####################\n",
      "RMSE: 0.9681023116607844\n",
      "####################\n",
      "1 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "!python compute_RMSE.py --path_to_predictions=preds.txt\n",
    "end = time.time()\n",
    "print(int(end - start), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture considerations\n",
    "\n",
    "The architecture of your model will impact the model's performance. Models with a greater capacity to learn may overfit, and generalize poorly to testing data. Models with a lower capacity to learn may underfit, and also generalize poorly to testing data. It's important to strike the right balance and tune your model's architecture and achieve optimal performance. Below we discuss several components of your model's architecture that may affect performance.\n",
    "\n",
    "#### Activation types\n",
    "\n",
    "To explore the effects of using different activation functions, we tested some of the most popular choices in deep learning: sigmoid, “rectifed linear units” (RELU), max(relu(x), 6) or RELU6, hyperbolic tangent (TANH), “exponential linear units” (ELU), leaky relu (LRELU), and “scaled exponential linear units” (SELU) on the 4 layer autoencoder with 128 units in each hidden layer. \n",
    "\n",
    "Because ratings are on the scale from 1 to 5, we keep last layer of the decoder linear for sigmoid and tanh-based models. In all other models activation function is applied in all layers. \n",
    "\n",
    "We found that on this task ELU, SELU and LRELU perform much better than SIGMOID, RELU, RELU6 and TANH. There are two properties which seems to separate activations which perform well from those which do not: a) non-zero negative part and b) unbounded positive part. Hence, we conclude, that in this setting these properties are important for successful training. Thus, we use SELU activation units and tune SELU-based networks for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going deeper\n",
    "\n",
    "There are two easy ways to increase a network's capacity to learn; we can make the network deeper by increasing the number of hidden layers, or we can make the network wider by increasing the number of neurons in each hidden layer. \n",
    "\n",
    "While making layers wider helps bring training loss down, adding more layers is often correlated with a network’s ability to generalize. In this set of experiments we show that this is indeed the case here. We choose small enough dimensionality (d = 128) for all hidden layers to easily avoid over-fitting and start adding more layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "Dropout was first proposed by G.E. Hinton in 2012. To be edited.\n",
    "\n",
    "This model, however, quickly over-fits if trained with no regularization. To regularize it, we tried several dropout values and, interestingly, very high values of drop probability (e.g. 0.8) turned out to be the best. We apply dropout on the encoder output only, e.g. $f(x) = decode(dropout(encode(x)))$. We tried applying dropout after every layer of the model but that stifled training convergence and did not improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense re-feeding\n",
    "\n",
    "Iterative dense re-feeding provides us with additional improvement in evaluation accuracy for our 6-layer-model: n, 512, 512, 1024,dp(0.8), 512, 512,n (referred to as Baseline below). Here each parameter denotes the number of inputs, hidden units, or outputs and dp(0.8) is a dropout layer with a drop probability of 0.8. Just applying output re-feeding did not have significant impact on the model performance. However, in conjunction with the higher learning rate, it did significantly increase the model performance. \n",
    "\n",
    "Note, that with this higher learning rate (0.005) but without dense re-feeding, the model started to diverge. Applying dense re-feeding and increasing the learning rate, allowed us to further improve the evaluation RMSE from 0.9167 to 0.9100. Picking a checkpoint with best evaluation RMSE and computing test RMSE gives as 0.9099, which we believe is significantly better than other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "6) Explore how different activation types effects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python infer.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python compute_RMSE.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Explore how changing the number of hidden layers effects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python infer.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python compute_RMSE.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Explore how increasing or decreasing the efficacy of dropout effects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python infer.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python compute_RMSE.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Explore how implementing dense re-feeding effects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python infer.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python compute_RMSE.py <insert parameters here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now trained Deep Autoencoders and used them to infer a user's preference for an item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
